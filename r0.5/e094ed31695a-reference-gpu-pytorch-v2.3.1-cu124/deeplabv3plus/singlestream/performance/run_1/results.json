{
    "args": {
        "accuracy": false,
        "audit_conf": "audit.config",
        "backend": "pytorch",
        "checkpoint": "/home/mlcuser/MLC/repos/local/cache/download-file_6d4d2eb9/latest_deeplabv3plus_resnet50_cognata_os16_it100000.pth/latest_deeplabv3plus_resnet50_cognata_os16_it100000.pth",
        "count": null,
        "dataset": "cognata",
        "dataset_path": "/home/mlcuser/MLC/repos/local/cache/download-file_289835b1/val_seg.tar.gz/val_seg",
        "debug": false,
        "device": "cuda",
        "dtype": "fp32",
        "find_peak_performance": false,
        "max_batchsize": 1,
        "max_latency": null,
        "model_name": "deeplabv3plus",
        "output": "/home/mlcuser/MLC/repos/local/cache/get-mlperf-inference-results-dir_7ecdd79f/valid_results/e094ed31695a-reference-gpu-pytorch-v2.3.1-cu124/deeplabv3plus/singlestream/performance/run_1",
        "performance_sample_count": 128,
        "profile": null,
        "qps": null,
        "samples_per_query": 8,
        "scenario": "SingleStream",
        "threads": 1,
        "time": null,
        "user_conf": "/home/mlcuser/MLC/repos/anandhu-eng@mlperf-automations/script/generate-mlperf-inference-user-conf/tmp/3f6ec12245744231861fb7558baf9421.conf"
    },
    "cmdline": "Namespace(dataset='cognata', dataset_path='/home/mlcuser/MLC/repos/local/cache/download-file_289835b1/val_seg.tar.gz/val_seg', profile=None, scenario='SingleStream', max_batchsize=1, threads=1, accuracy=False, find_peak_performance=False, backend='pytorch', model_name='deeplabv3plus', output='/home/mlcuser/MLC/repos/local/cache/get-mlperf-inference-results-dir_7ecdd79f/valid_results/e094ed31695a-reference-gpu-pytorch-v2.3.1-cu124/deeplabv3plus/singlestream/performance/run_1', qps=None, checkpoint='/home/mlcuser/MLC/repos/local/cache/download-file_6d4d2eb9/latest_deeplabv3plus_resnet50_cognata_os16_it100000.pth/latest_deeplabv3plus_resnet50_cognata_os16_it100000.pth', dtype='fp32', device='cuda', user_conf='/home/mlcuser/MLC/repos/anandhu-eng@mlperf-automations/script/generate-mlperf-inference-user-conf/tmp/3f6ec12245744231861fb7558baf9421.conf', audit_conf='audit.config', time=None, count=None, debug=False, performance_sample_count=128, max_latency=None, samples_per_query=8)",
    "runtime": "python-SUT",
    "time": 1748542339,
    "version": "2.3.1+cu121"
}